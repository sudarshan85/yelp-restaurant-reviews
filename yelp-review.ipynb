{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern NLP Tutorial\n",
    "This notebook is following the tutorial from PyData 2016 by Patrick Harrision titled \"Modern NLP in Python\". It involves processing Yelp restaurant reviews, modeling topics from them, visualizing the topics, and creating and visualizing word vectors. The original notebook can be found [here](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). The accompanying video from PyData can be found [here](https://youtu.be/6zm9NC9uRkk). The academic dataset used in the notebook can be downloaded from [here](https://app.dominodatalab.com/mtldata/yackathon/browse/yelp_dataset_challenge_academic_dataset). The entire Yelp dataset can be found [here](https://www.yelp.com/dataset).\n",
    "\n",
    "This notebook works with the entire dataset and not the academic dataset because when I started working on this, I couldn't find the academic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and set data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:13.197777Z",
     "start_time": "2018-03-09T00:29:11.956677Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:13.233147Z",
     "start_time": "2018-03-09T00:29:13.199310Z"
    }
   },
   "outputs": [],
   "source": [
    "academic = True\n",
    "prefix = 'yelp_academic_dataset_' if academic else ''\n",
    "folder = 'yelp-academic' if academic else 'yelp-full'\n",
    "\n",
    "data_directory = os.path.join('/mnt/Data/ml/datasets/yelp-dataset/' + folder)\n",
    "businesses_filepath = os.path.join(data_directory, prefix + 'business.json')\n",
    "review_json_filepath = os.path.join(data_directory, prefix + 'review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "tsne_filepath = os.path.join(intermediate_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Write out the review file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the business json file and go through each business. Count how many restaurants are present and get their ids.\n",
    "\n",
    "Total number of restaurants in yelp-academic: 21,892\n",
    "\n",
    "Total number of restaurants in yelp-full: 54,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:47:34.679558Z",
     "start_time": "2018-03-07T12:47:28.072380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import get_restaurant_ids\n",
    "    \n",
    "restaurant_ids = get_restaurant_ids(businesses_filepath)\n",
    "print(f'{len(restaurant_ids)} restaurants in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the reviews of each restaurant **ONE LINE PER REVIEW** into the reviews file. This is done by escaping the newline character and replacing it with raw '\\n' and adding a '\\n' at the end to specify a newline\n",
    "\n",
    "Number of reviews in yelp-academic: 990,627\n",
    "\n",
    "Number of reviews in yelp-full: 3,221,419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:46:15.627899Z",
     "start_time": "2018-03-07T12:45:09.565543Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_review_file\n",
    "\n",
    "review_count = write_review_file(review_txt_filepath, review_json_filepath, restaurant_ids)\n",
    "print(f'Text from {review_count} reviews written to new txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:33.553286Z",
     "start_time": "2018-03-09T00:29:15.112127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Review\n",
    "Grab a sample review and analyze various aspects of SpaCy using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.994373Z",
     "start_time": "2018-03-08T02:49:15.292064Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_reveiw = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:26:27.996891Z",
     "start_time": "2018-03-08T02:26:27.986659Z"
    }
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num+1}:')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:27:34.344465Z",
     "start_time": "2018-03-08T02:27:34.337516Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num+1}: {entity} - {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:51:19.540873Z",
     "start_time": "2018-03-08T02:51:19.472426Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_attrs = [(token.text,\n",
    "                token.pos_,\n",
    "                token.lemma_,\n",
    "                token.shape_,\n",
    "                token.prob,\n",
    "                token.text in STOP_WORDS,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attrs, columns=['text', 'pos', 'lemma', 'shape', 'log_prob',\n",
    "                                       'stop?', 'punct?', 'whitespace?', 'number?',\n",
    "                                        'out of vocab?'])\n",
    "df.loc[:, 'stop?':'out of vocab?'] = (df.loc[:, 'stop?':'out of vocab?']\n",
    "                                     .applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram write file (once)\n",
    "Get sentences from each review and write out the unigram file. This should be done only once.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,146,794  \n",
    "Time taken to process yelp-academic: 5h 34m 46s\n",
    "\n",
    "Number of sentences in yelp-full: 30,392,900  \n",
    "Time taken to process yelp-full: 16h 1m 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T22:16:34.525401Z",
     "start_time": "2018-03-07T16:41:48.872601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_unigram_sents\n",
    "\n",
    "sentence_count = write_unigram_sents(unigram_sentences_filepath, review_txt_filepath, nlp)\n",
    "print(f'{sentence_count} sentences written to {unigram_sentences_filepath} file')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:52:45.313094Z",
     "start_time": "2018-03-08T02:52:45.309400Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:53:20.636728Z",
     "start_time": "2018-03-08T02:53:20.602844Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:31:51.551923Z",
     "start_time": "2018-03-08T16:31:51.548675Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:32:15.630048Z",
     "start_time": "2018-03-08T16:32:15.621292Z"
    }
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Bigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into two-word phrases. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 3s  \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 13m 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:44:52.784268Z",
     "start_time": "2018-03-08T12:44:52.780429Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:58:27.334648Z",
     "start_time": "2018-03-08T12:45:10.287325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram sentences write file (once)\n",
    "After learning the bigram phrase model, we feed in the individual sentences from unigram_sentences to find possible bigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:31.895533Z",
     "start_time": "2018-03-08T16:05:26.892574Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the bigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 12m 45s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 36m 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:18:16.922145Z",
     "start_time": "2018-03-08T16:05:31.896940Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(bigram_sentences_filepath, unigram_sentences, bigram_model)\n",
    "print(f'{sentence_count} sentences written to {bigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:33:56.758162Z",
     "start_time": "2018-03-08T16:33:56.755535Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:34:31.379629Z",
     "start_time": "2018-03-08T16:34:31.371560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into three-word phrases based on the input from bigram sentences. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 35s \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 12m 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:26:27.495322Z",
     "start_time": "2018-03-08T17:26:27.491511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:38:41.404243Z",
     "start_time": "2018-03-08T17:26:27.496573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram sentences write file (once)\n",
    "After learning the trigram phrase model, we feed in the individual sentences from bigram_sentences to find possible triigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:39:48.582668Z",
     "start_time": "2018-03-08T19:39:42.963523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the trigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 11m 54s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 35m 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:51:42.088160Z",
     "start_time": "2018-03-08T19:39:48.584521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(trigram_sentences_filepath, bigram_sentences, trigram_model)\n",
    "print(f'{sentence_count} sentences written to {trigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Trigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:45:15.016247Z",
     "start_time": "2018-03-08T20:45:15.014275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:46:19.149285Z",
     "start_time": "2018-03-08T20:46:19.143003Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 300, 350):\n",
    "    print(u' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating full reviews file\n",
    "\n",
    "Now we will generate the full complete text of reviews which would have normalized text, no stopwords, and second order phrases (trigram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:46.083706Z",
     "start_time": "2018-03-09T00:29:35.102697Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of reviews in yelp-academic: 991,714  \n",
    "Time taken to write reviews in yelp-academic: 5h 48m 52s  \n",
    "The number of reviews in the original reviews file is 990,627. The trigram transformed reviews have 1,087 reviews more than the original reviews. I am not sure where the increased number of reviews came from.\n",
    "\n",
    "Number of reviews in yelp-full: 3,223,214  \n",
    "Time taken to write reviews in yelp-full: 16h 55m 33s\n",
    "The number of reviews in the original reviews file is 3,221,419. The trigram transformed reviews have 1,795 reviews more than the original reviews. I am not sure where the increased number of reviews came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T06:18:39.829342Z",
     "start_time": "2018-03-09T00:29:47.671374Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_trigram_review\n",
    "\n",
    "review_count = write_trigram_review(trigram_reviews_filepath, review_txt_filepath, bigram_model, trigram_model,\n",
    "                                   nlp)\n",
    "print(f'{review_count} reviews written to {trigram_reviews_filepath}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
