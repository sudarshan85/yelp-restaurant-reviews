{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern NLP Tutorial\n",
    "This notebook is following the tutorial from PyData 2016 by Patrick Harrision titled \"Modern NLP in Python\". It involves processing Yelp restaurant reviews, modeling topics from them, visualizing the topics, and creating and visualizing word vectors. The original notebook can be found [here](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). The accompanying video from PyData can be found [here](https://youtu.be/6zm9NC9uRkk). The academic dataset used in the notebook can be downloaded from [here](https://app.dominodatalab.com/mtldata/yackathon/browse/yelp_dataset_challenge_academic_dataset). The entire Yelp dataset can be found [here](https://www.yelp.com/dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and set data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T16:17:16.433730Z",
     "start_time": "2018-03-10T16:17:16.424119Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T17:31:17.789452Z",
     "start_time": "2018-03-10T17:31:17.755699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Toggle this variable to choose between the full and academic yelp datasets\n",
    "academic = True\n",
    "prefix = 'yelp_academic_dataset_' if academic else ''\n",
    "folder = 'yelp-academic' if academic else 'yelp-full'\n",
    "\n",
    "data_directory = os.path.join('/mnt/Data/ml/datasets/yelp-dataset/' + folder)\n",
    "businesses_filepath = os.path.join(data_directory, prefix + 'business.json')\n",
    "review_json_filepath = os.path.join(data_directory, prefix + 'review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "tsne_filepath = os.path.join(intermediate_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Write out the review file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the business json file and go through each business. Count how many restaurants are present and get their ids.\n",
    "\n",
    "Total number of restaurants in yelp-academic: 21,892\n",
    "\n",
    "Total number of restaurants in yelp-full: 54,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:47:34.679558Z",
     "start_time": "2018-03-07T12:47:28.072380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import get_restaurant_ids\n",
    "    \n",
    "restaurant_ids = get_restaurant_ids(businesses_filepath)\n",
    "print(f'{len(restaurant_ids)} restaurants in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the reviews of each restaurant **ONE LINE PER REVIEW** into the reviews file. This is done by escaping the newline character and replacing it with raw '\\n' and adding a '\\n' at the end to specify a newline\n",
    "\n",
    "Number of reviews in yelp-academic: 990,627\n",
    "\n",
    "Number of reviews in yelp-full: 3,221,419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:46:15.627899Z",
     "start_time": "2018-03-07T12:45:09.565543Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_review_file\n",
    "\n",
    "review_count = write_review_file(review_txt_filepath, review_json_filepath, restaurant_ids)\n",
    "print(f'Text from {review_count} reviews written to new txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SpaCy Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:33.553286Z",
     "start_time": "2018-03-09T00:29:15.112127Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sample Review\n",
    "Grab a sample review and analyze various aspects of SpaCy using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.994373Z",
     "start_time": "2018-03-08T02:49:15.292064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_reveiw = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:26:27.996891Z",
     "start_time": "2018-03-08T02:26:27.986659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num+1}:')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:27:34.344465Z",
     "start_time": "2018-03-08T02:27:34.337516Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num+1}: {entity} - {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:51:19.540873Z",
     "start_time": "2018-03-08T02:51:19.472426Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_attrs = [(token.text,\n",
    "                token.pos_,\n",
    "                token.lemma_,\n",
    "                token.shape_,\n",
    "                token.prob,\n",
    "                token.text in STOP_WORDS,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attrs, columns=['text', 'pos', 'lemma', 'shape', 'log_prob',\n",
    "                                       'stop?', 'punct?', 'whitespace?', 'number?',\n",
    "                                        'out of vocab?'])\n",
    "df.loc[:, 'stop?':'out of vocab?'] = (df.loc[:, 'stop?':'out of vocab?']\n",
    "                                     .applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Unigram write file (once)\n",
    "Get sentences from each review and write out the unigram file. This should be done only once.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,146,794  \n",
    "Time taken to process yelp-academic: 5h 34m 46s\n",
    "\n",
    "Number of sentences in yelp-full: 30,392,900  \n",
    "Time taken to process yelp-full: 16h 1m 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T22:16:34.525401Z",
     "start_time": "2018-03-07T16:41:48.872601Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_unigram_sents\n",
    "\n",
    "sentence_count = write_unigram_sents(unigram_sentences_filepath, review_txt_filepath, nlp)\n",
    "print(f'{sentence_count} sentences written to {unigram_sentences_filepath} file')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:52:45.313094Z",
     "start_time": "2018-03-08T02:52:45.309400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:53:20.636728Z",
     "start_time": "2018-03-08T02:53:20.602844Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Unigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:31:51.551923Z",
     "start_time": "2018-03-08T16:31:51.548675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:32:15.630048Z",
     "start_time": "2018-03-08T16:32:15.621292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into two-word phrases. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 3s  \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 13m 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:44:52.784268Z",
     "start_time": "2018-03-08T12:44:52.780429Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:58:27.334648Z",
     "start_time": "2018-03-08T12:45:10.287325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Bigram sentences write file (once)\n",
    "After learning the bigram phrase model, we feed in the individual sentences from unigram_sentences to find possible bigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:31.895533Z",
     "start_time": "2018-03-08T16:05:26.892574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the bigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 12m 45s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 36m 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:18:16.922145Z",
     "start_time": "2018-03-08T16:05:31.896940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(bigram_sentences_filepath, unigram_sentences, bigram_model)\n",
    "print(f'{sentence_count} sentences written to {bigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Bigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:33:56.758162Z",
     "start_time": "2018-03-08T16:33:56.755535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:34:31.379629Z",
     "start_time": "2018-03-08T16:34:31.371560Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into three-word phrases based on the input from bigram sentences. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 35s \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 12m 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:26:27.495322Z",
     "start_time": "2018-03-08T17:26:27.491511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:38:41.404243Z",
     "start_time": "2018-03-08T17:26:27.496573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram sentences write file (once)\n",
    "After learning the trigram phrase model, we feed in the individual sentences from bigram_sentences to find possible triigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:39:48.582668Z",
     "start_time": "2018-03-08T19:39:42.963523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the trigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 11m 54s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 35m 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:51:42.088160Z",
     "start_time": "2018-03-08T19:39:48.584521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(trigram_sentences_filepath, bigram_sentences, trigram_model)\n",
    "print(f'{sentence_count} sentences written to {trigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Trigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:45:15.016247Z",
     "start_time": "2018-03-08T20:45:15.014275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:46:19.149285Z",
     "start_time": "2018-03-08T20:46:19.143003Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 300, 350):\n",
    "    print(u' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generating full reviews file\n",
    "\n",
    "Now we will generate the full complete text of reviews which would have normalized text, no stopwords, and second order phrases (trigram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:46.083706Z",
     "start_time": "2018-03-09T00:29:35.102697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Number of reviews in yelp-academic: 991,714  \n",
    "Time taken to write reviews in yelp-academic: 5h 48m 52s  \n",
    "The number of reviews in the original reviews file is 990,627. The trigram transformed reviews have 1,087 reviews more than the original reviews. I am not sure where the increased number of reviews came from.\n",
    "\n",
    "Number of reviews in yelp-full: 3,223,214  \n",
    "Time taken to write reviews in yelp-full: 16h 55m 33s\n",
    "The number of reviews in the original reviews file is 3,221,419. The trigram transformed reviews have 1,795 reviews more than the original reviews. I am not sure where the increased number of reviews came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T06:18:39.829342Z",
     "start_time": "2018-03-09T00:29:47.671374Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_trigram_review\n",
    "\n",
    "review_count = write_trigram_review(trigram_reviews_filepath, review_txt_filepath, bigram_model, trigram_model,\n",
    "                                   nlp)\n",
    "print(f'{review_count} reviews written to {trigram_reviews_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Review File example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:11:13.234149Z",
     "start_time": "2018-03-10T13:11:13.074638Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import line_review\n",
    "\n",
    "print(\"Original:\")\n",
    "print()\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 4352, 4353):\n",
    "    print(review)\n",
    "\n",
    "print(\"----\")\n",
    "print()\n",
    "print(\"Transformed:\")\n",
    "print()\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 4352, 4353):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)\n",
    "\n",
    "We want to put the reviews into different representing different things. The groups are essentially the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a full vocabulary of the corpus to be modeled using gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html)  class.\n",
    "\n",
    "Time taken to create yelp-academic dictionary: 1m 12.5s \n",
    "\n",
    "Time taken to create yelp-full dictionary: 3m 5s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:39:17.992065Z",
     "start_time": "2018-03-10T13:35:27.833811Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bag-of-words model\n",
    "\n",
    "Using the dictionary created above (which is just a mapping of words to integer ID's we create a bag-of-words model where each review is represented by the coutns of distinct terms in it.\n",
    "\n",
    "Time taken to create and save yelp-academic BOW model: 2m 22s \n",
    "\n",
    "Time taken to create and save yelp-full BOW model: 6m 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T14:06:30.015973Z",
     "start_time": "2018-03-10T13:59:55.637781Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_fns import trigram_bow_generator\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)\n",
    "\n",
    "MmCorpus.serialize(trigram_bow_filepath,\n",
    "                   trigram_bow_generator(trigram_reviews_filepath, trigram_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model Generation\n",
    "\n",
    "Create a LDA model with 50 topics.\n",
    "\n",
    "Time taken to create and save yelp-academic LDA model: 18m 51s \n",
    "\n",
    "Time taken to create and save yelp-full LDA model: 54m 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T15:57:04.064390Z",
     "start_time": "2018-03-10T15:03:02.685878Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "lda.save(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yelp-academic dataset did not have a common theme within the topics but the yelp-full had one. This indicates as always more data is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T16:18:35.160022Z",
     "start_time": "2018-03-10T16:18:35.004823Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_fns import explore_topic\n",
    "\n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T16:22:04.896016Z",
     "start_time": "2018-03-10T16:22:04.880567Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_topic(lda, topic_number=np.random.randint(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic visualization with pyLDvis\n",
    "Prepare the visualization file.\n",
    "\n",
    "Time taken to prepare yelp-academic viz file:\n",
    "\n",
    "Time taken to prepare yelp-ful viz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T17:40:37.592417Z",
     "start_time": "2018-03-10T17:40:37.562704Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)\n",
    "\n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T17:26:28.669331Z",
     "start_time": "2018-03-10T16:28:10.676730Z"
    }
   },
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus, trigram_dictionary)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
