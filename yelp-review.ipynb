{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern NLP Tutorial\n",
    "This notebook is following the tutorial from PyData 2016 by Patrick Harrision titled \"Modern NLP in Python\". It involves processing Yelp restaurant reviews, modeling topics from them, visualizing the topics, and creating and visualizing word vectors. The original notebook can be found [here](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). The accompanying video from PyData can be found [here](https://youtu.be/6zm9NC9uRkk). The academic dataset used in the notebook can be downloaded from [here](https://app.dominodatalab.com/mtldata/yackathon/browse/yelp_dataset_challenge_academic_dataset). The entire Yelp dataset can be found [here](https://www.yelp.com/dataset).\n",
    "\n",
    "This notebook works with the entire dataset and not the academic dataset because when I started working on this, I couldn't find the academic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and set data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:08.999148Z",
     "start_time": "2018-03-08T16:05:08.312767Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:09.036173Z",
     "start_time": "2018-03-08T16:05:09.001020Z"
    }
   },
   "outputs": [],
   "source": [
    "academic = True\n",
    "prefix = 'yelp_academic_dataset_' if academic else ''\n",
    "folder = 'yelp-academic' if academic else 'yelp-full'\n",
    "\n",
    "data_directory = os.path.join('/mnt/Data/ml/datasets/yelp-dataset/' + folder)\n",
    "businesses_filepath = os.path.join(data_directory, prefix + 'business.json')\n",
    "review_json_filepath = os.path.join(data_directory, prefix + 'review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "tsne_filepath = os.path.join(intermediate_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Write out the review file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the business json file and go through each business. Count how many restaurants are present and get their ids.\n",
    "\n",
    "Total number of restaurants in yelp-academic: 21,892\n",
    "\n",
    "Total number of restaurants in yelp-full: 54,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:47:34.679558Z",
     "start_time": "2018-03-07T12:47:28.072380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from file_manip import get_restaurant_ids\n",
    "    \n",
    "restaurant_ids = get_restaurant_ids(businesses_filepath)\n",
    "print(f'{len(restaurant_ids)} restaurants in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the reviews of each restaurant **ONE LINE PER REVIEW** into the reviews file. This is done by escaping the newline character and replacing it with raw '\\n' and adding a '\\n' at the end to specify a newline\n",
    "\n",
    "Number of reviews in yelp-academic: 990,627\n",
    "\n",
    "Number of reviews in yelp-full: 3,221,419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:46:15.627899Z",
     "start_time": "2018-03-07T12:45:09.565543Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from file_manip import write_review_file\n",
    "review_count = write_review_file(review_txt_filepath, review_json_filepath, restaurant_ids)\n",
    "print(f'Text from {review_count} reviews written to new txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:26.891230Z",
     "start_time": "2018-03-08T16:05:11.096076Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sample Review\n",
    "Grab a sample review and analyze various aspects of SpaCy using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.994373Z",
     "start_time": "2018-03-08T02:49:15.292064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_reveiw = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:26:27.996891Z",
     "start_time": "2018-03-08T02:26:27.986659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num+1}:')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:27:34.344465Z",
     "start_time": "2018-03-08T02:27:34.337516Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num+1}: {entity} - {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:51:19.540873Z",
     "start_time": "2018-03-08T02:51:19.472426Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_attrs = [(token.text,\n",
    "                token.pos_,\n",
    "                token.lemma_,\n",
    "                token.shape_,\n",
    "                token.prob,\n",
    "                token.text in STOP_WORDS,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attrs, columns=['text', 'pos', 'lemma', 'shape', 'log_prob',\n",
    "                                       'stop?', 'punct?', 'whitespace?', 'number?',\n",
    "                                        'out of vocab?'])\n",
    "df.loc[:, 'stop?':'out of vocab?'] = (df.loc[:, 'stop?':'out of vocab?']\n",
    "                                     .applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Unigram write file (once)\n",
    "Get sentences from each review and write out the unigram file. This should be done only once.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,146,794  \n",
    "Time taken to process yelp-academic: 5h 34m 46s\n",
    "\n",
    "Number of sentences in yelp-full: 30,392,900  \n",
    "Time taken to process yelp-full: 16h 1m 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T22:16:34.525401Z",
     "start_time": "2018-03-07T16:41:48.872601Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from file_manip import write_unigram_sents\n",
    "\n",
    "sentence_count = write_unigram_sents(unigram_sentences_filepath, review_txt_filepath, nlp)\n",
    "print(f'{sentence_count} sentences written to {unigram_sentences_filepath} file')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:52:45.313094Z",
     "start_time": "2018-03-08T02:52:45.309400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:53:20.636728Z",
     "start_time": "2018-03-08T02:53:20.602844Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Bigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into two-word phrases. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 3s  \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 13m 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:44:52.784268Z",
     "start_time": "2018-03-08T12:44:52.780429Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:58:27.334648Z",
     "start_time": "2018-03-08T12:45:10.287325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram sentences write file (once)\n",
    "After learning the bigram phrase model, we feed in the individual sentences from unigram_sentences to find possible bigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:31.895533Z",
     "start_time": "2018-03-08T16:05:26.892574Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the bigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 12m 45s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 36m 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:18:16.922145Z",
     "start_time": "2018-03-08T16:05:31.896940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudarshan/anaconda3/envs/yelp/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10109973 sentences written to /mnt/Data/ml/datasets/yelp-dataset/yelp-academic/intermediate/bigram_sentences_all.txt\n"
     ]
    }
   ],
   "source": [
    "from file_manip import write_sents\n",
    "\n",
    "sentence_count = write_sents(bigram_sentences_filepath, unigram_sentences, bigram_model)\n",
    "print(f'{sentence_count} sentences written to {bigram_sentences_filepath}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
