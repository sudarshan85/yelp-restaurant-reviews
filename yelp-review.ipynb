{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern NLP Tutorial\n",
    "This notebook is following the tutorial from PyData 2016 by Patrick Harrision titled \"Modern NLP in Python\". It involves processing Yelp restaurant reviews, modeling topics from them, visualizing the topics, and creating and visualizing word vectors. The original notebook can be found [here](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). The accompanying video from PyData can be found [here](https://youtu.be/6zm9NC9uRkk). The academic dataset used in the notebook can be downloaded from [here](https://app.dominodatalab.com/mtldata/yackathon/browse/yelp_dataset_challenge_academic_dataset). The entire Yelp dataset can be found [here](https://www.yelp.com/dataset).\n",
    "\n",
    "This notebook works with the entire dataset and not the academic dataset because when I started working on this, I couldn't find the academic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and set data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:48:52.619452Z",
     "start_time": "2018-03-08T02:48:52.614350Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:48:55.028161Z",
     "start_time": "2018-03-08T02:48:54.987377Z"
    }
   },
   "outputs": [],
   "source": [
    "academic = True\n",
    "prefix = 'yelp_academic_dataset_' if academic else ''\n",
    "folder = 'yelp-academic' if academic else 'yelp-full'\n",
    "\n",
    "data_directory = os.path.join('/mnt/Data/ml/datasets/yelp-dataset/' + folder)\n",
    "businesses_filepath = os.path.join(data_directory, prefix + 'business.json')\n",
    "review_json_filepath = os.path.join(data_directory, prefix + 'review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "tsne_filepath = os.path.join(intermediate_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Write out the review file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the business json file and go through each business. Count how many restaurants are present and get their ids.\n",
    "\n",
    "Total number of restaurants in yelp-academic: 21,892\n",
    "\n",
    "Total number of restaurants in yelp-full: 54,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:47:34.679558Z",
     "start_time": "2018-03-07T12:47:28.072380Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54618 restaurants in the dataset\n"
     ]
    }
   ],
   "source": [
    "from file_manip import get_restaurant_ids\n",
    "    \n",
    "restaurant_ids = get_restaurant_ids(businesses_filepath)\n",
    "print(f'{len(restaurant_ids)} restaurants in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the reviews of each restaurant **ONE LINE PER REVIEW** into the reviews file. This is done by escaping the newline character and replacing it with raw '\\n' and adding a '\\n' at the end to specify a newline\n",
    "\n",
    "Number of reviews in yelp-academic: 990,627\n",
    "\n",
    "Number of reviews in yelp-full: 3,221,419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:46:15.627899Z",
     "start_time": "2018-03-07T12:45:09.565543Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 990627 reviews written to new txt file\n",
      "CPU times: user 54.9 s, sys: 2.58 s, total: 57.4 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "from file_manip import write_review_file\n",
    "review_count = write_review_file(review_txt_filepath, review_json_filepath, restaurant_ids)\n",
    "print(f'Text from {review_count} reviews written to new txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.290832Z",
     "start_time": "2018-03-08T02:48:59.675606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sample Review\n",
    "Grab a sample review and analyze various aspects of SpaCy using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.994373Z",
     "start_time": "2018-03-08T02:49:15.292064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_reveiw = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:26:27.996891Z",
     "start_time": "2018-03-08T02:26:27.986659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num+1}:')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:27:34.344465Z",
     "start_time": "2018-03-08T02:27:34.337516Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num+1}: {entity} - {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:51:19.540873Z",
     "start_time": "2018-03-08T02:51:19.472426Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_attrs = [(token.text,\n",
    "                token.pos_,\n",
    "                token.lemma_,\n",
    "                token.shape_,\n",
    "                token.prob,\n",
    "                token.text in STOP_WORDS,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attrs, columns=['text', 'pos', 'lemma', 'shape', 'log_prob',\n",
    "                                       'stop?', 'punct?', 'whitespace?', 'number?',\n",
    "                                        'out of vocab?'])\n",
    "df.loc[:, 'stop?':'out of vocab?'] = (df.loc[:, 'stop?':'out of vocab?']\n",
    "                                     .applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Unigram write file (once)\n",
    "Get sentences from each review and write out the unigram file. This should be done only once.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,146,794\n",
    "Time taken to process yelp-academic: 5h 34m 46s\n",
    "\n",
    "Number of sentences in yelp-full: 30,392,900\n",
    "Time taken to process yelp-full: 16h 1m 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T22:16:34.525401Z",
     "start_time": "2018-03-07T16:41:48.872601Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10146794 sentences written to /mnt/Data/ml/datasets/yelp-dataset/yelp-academic/intermediate/unigram_sentences_all.txt file\n"
     ]
    }
   ],
   "source": [
    "from file_manip import write_unigram_sents\n",
    "\n",
    "sentence_count = write_unigram_sents(unigram_sentences_filepath, review_txt_filepath, nlp)\n",
    "print(f'{sentence_count} sentences written to {unigram_sentences_filepath} file')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:52:45.313094Z",
     "start_time": "2018-03-08T02:52:45.309400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:53:20.636728Z",
     "start_time": "2018-03-08T02:53:20.602844Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- have never see a restaurant that have a frowning brownie a.k.a frownie as -PRON- icon mascot or spokesperson\n",
      "king 's family restaurant have surprise -PRON- with this\n",
      "-PRON- think -PRON- may be in direct dialogue with eat n park smile cookie funny funny odd not funny ha ha\n",
      "-PRON- be seat rather quickly by the manager\n",
      "very nice people work here -PRON- be happy to find that even though -PRON- have close a section the server be willing to stay longer to serve -PRON- -PRON- dinner\n",
      "-PRON- chat with the server a little bit\n",
      "-PRON- be quite engaging and then move on to order -PRON- meal\n",
      "tea pepsi\n",
      "open face hot turkey sandwich with mash potato and gravy buffalo chicken strip with mash potato and macaroni n cheese\n",
      "-PRON- drink arrive and -PRON- server converse with -PRON- some more about the area\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Bigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into two-word phrases. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 3m 55s  \n",
    "Time taken to save yelp-academic bigram model: 8s\n",
    "\n",
    "Time taken to generate yelp-full bigram model: 3m 55s  \n",
    "Time taken to save yelp-full bigram model: 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T03:00:14.379013Z",
     "start_time": "2018-03-08T02:56:19.270917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T03:00:14.379013Z",
     "start_time": "2018-03-08T02:56:19.270917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram sentences write file (once)\n",
    "After learning the bigram phrase model, we feed in the individual sentences from unigram_sentences to find possible bigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:32:50.517135Z",
     "start_time": "2018-03-08T12:32:45.565836Z"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_manip import write_sents\n",
    "\n",
    "sentence_count = write_sents(bigram_sentences_filepath, unigram_sentences, bigram_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
