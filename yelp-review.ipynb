{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern NLP Tutorial\n",
    "This notebook is following the tutorial from PyData 2016 by Patrick Harrision titled \"Modern NLP in Python\". It involves processing Yelp restaurant reviews, modeling topics from them, visualizing the topics, and creating and visualizing word vectors. The original notebook can be found [here](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb). The accompanying video from PyData can be found [here](https://youtu.be/6zm9NC9uRkk). The academic dataset used in the notebook can be downloaded from [here](https://app.dominodatalab.com/mtldata/yackathon/browse/yelp_dataset_challenge_academic_dataset). The entire Yelp dataset can be found [here](https://www.yelp.com/dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and set data directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:47:01.304234Z",
     "start_time": "2018-03-10T13:47:01.292946Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:40:36.050566Z",
     "start_time": "2018-03-10T13:40:36.013958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Toggle this variable to choose between the full and academic yelp datasets\n",
    "academic = True\n",
    "prefix = 'yelp_academic_dataset_' if academic else ''\n",
    "folder = 'yelp-academic' if academic else 'yelp-full'\n",
    "\n",
    "data_directory = os.path.join('/mnt/Data/ml/datasets/yelp-dataset/' + folder)\n",
    "businesses_filepath = os.path.join(data_directory, prefix + 'business.json')\n",
    "review_json_filepath = os.path.join(data_directory, prefix + 'review.json')\n",
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory, 'review_text_all.txt')\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all.txt')\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')\n",
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory, 'trigram_transformed_reviews_all.txt')\n",
    "\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')\n",
    "\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "tsne_filepath = os.path.join(intermediate_directory, 'tsne_model')\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory, 'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Write out the review file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the business json file and go through each business. Count how many restaurants are present and get their ids.\n",
    "\n",
    "Total number of restaurants in yelp-academic: 21,892\n",
    "\n",
    "Total number of restaurants in yelp-full: 54,618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:47:34.679558Z",
     "start_time": "2018-03-07T12:47:28.072380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import get_restaurant_ids\n",
    "    \n",
    "restaurant_ids = get_restaurant_ids(businesses_filepath)\n",
    "print(f'{len(restaurant_ids)} restaurants in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the reviews of each restaurant **ONE LINE PER REVIEW** into the reviews file. This is done by escaping the newline character and replacing it with raw '\\n' and adding a '\\n' at the end to specify a newline\n",
    "\n",
    "Number of reviews in yelp-academic: 990,627\n",
    "\n",
    "Number of reviews in yelp-full: 3,221,419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T12:46:15.627899Z",
     "start_time": "2018-03-07T12:45:09.565543Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_review_file\n",
    "\n",
    "review_count = write_review_file(review_txt_filepath, review_json_filepath, restaurant_ids)\n",
    "print(f'Text from {review_count} reviews written to new txt file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:33.553286Z",
     "start_time": "2018-03-09T00:29:15.112127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Review\n",
    "Grab a sample review and analyze various aspects of SpaCy using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:49:15.994373Z",
     "start_time": "2018-03-08T02:49:15.292064Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_reveiw = sample_review.replace('\\\\n', '\\n')\n",
    "    \n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:26:27.996891Z",
     "start_time": "2018-03-08T02:26:27.986659Z"
    }
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num+1}:')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:27:34.344465Z",
     "start_time": "2018-03-08T02:27:34.337516Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num+1}: {entity} - {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:51:19.540873Z",
     "start_time": "2018-03-08T02:51:19.472426Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_attrs = [(token.text,\n",
    "                token.pos_,\n",
    "                token.lemma_,\n",
    "                token.shape_,\n",
    "                token.prob,\n",
    "                token.text in STOP_WORDS,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attrs, columns=['text', 'pos', 'lemma', 'shape', 'log_prob',\n",
    "                                       'stop?', 'punct?', 'whitespace?', 'number?',\n",
    "                                        'out of vocab?'])\n",
    "df.loc[:, 'stop?':'out of vocab?'] = (df.loc[:, 'stop?':'out of vocab?']\n",
    "                                     .applymap(lambda x: u'Yes' if x else u''))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Unigram write file (once)\n",
    "Get sentences from each review and write out the unigram file. This should be done only once.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,146,794  \n",
    "Time taken to process yelp-academic: 5h 34m 46s\n",
    "\n",
    "Number of sentences in yelp-full: 30,392,900  \n",
    "Time taken to process yelp-full: 16h 1m 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T22:16:34.525401Z",
     "start_time": "2018-03-07T16:41:48.872601Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_unigram_sents\n",
    "\n",
    "sentence_count = write_unigram_sents(unigram_sentences_filepath, review_txt_filepath, nlp)\n",
    "print(f'{sentence_count} sentences written to {unigram_sentences_filepath} file')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:52:45.313094Z",
     "start_time": "2018-03-08T02:52:45.309400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T02:53:20.636728Z",
     "start_time": "2018-03-08T02:53:20.602844Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Unigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:31:51.551923Z",
     "start_time": "2018-03-08T16:31:51.548675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:32:15.630048Z",
     "start_time": "2018-03-08T16:32:15.621292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into two-word phrases. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 3s  \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 13m 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:44:52.784268Z",
     "start_time": "2018-03-08T12:44:52.780429Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:58:27.334648Z",
     "start_time": "2018-03-08T12:45:10.287325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Bigram sentences write file (once)\n",
    "After learning the bigram phrase model, we feed in the individual sentences from unigram_sentences to find possible bigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:05:31.895533Z",
     "start_time": "2018-03-08T16:05:26.892574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the bigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 12m 45s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 36m 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:18:16.922145Z",
     "start_time": "2018-03-08T16:05:31.896940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(bigram_sentences_filepath, unigram_sentences, bigram_model)\n",
    "print(f'{sentence_count} sentences written to {bigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Bigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:33:56.758162Z",
     "start_time": "2018-03-08T16:33:56.755535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T16:34:31.379629Z",
     "start_time": "2018-03-08T16:34:31.371560Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram Phrase model create and save (once)\n",
    "We learn a phrase model that will link individual words into three-word phrases based on the input from bigram sentences. The model is saved after generation.\n",
    "\n",
    "Time taken to generate yelp-academic bigram model: 4m 35s \n",
    "\n",
    "Time taken to generate yelp-full bigram model: 12m 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:26:27.495322Z",
     "start_time": "2018-03-08T17:26:27.491511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T17:38:41.404243Z",
     "start_time": "2018-03-08T17:26:27.496573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Trigram sentences write file (once)\n",
    "After learning the trigram phrase model, we feed in the individual sentences from bigram_sentences to find possible triigram phrases. If found, gensim will automatically join them with an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:39:48.582668Z",
     "start_time": "2018-03-08T19:39:42.963523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write out the trigram sentences to disk.\n",
    "\n",
    "Number of sentences in yelp-academic: 10,109,973  \n",
    "Time taken to process yelp-academic: 11m 54s\n",
    "\n",
    "Number of sentences in yelp-full: 30,301,195  \n",
    "Time taken to process yelp-full: 35m 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:51:42.088160Z",
     "start_time": "2018-03-08T19:39:48.584521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_sents\n",
    "\n",
    "sentence_count = write_sents(trigram_sentences_filepath, bigram_sentences, trigram_model)\n",
    "print(f'{sentence_count} sentences written to {trigram_sentences_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Trigram Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:45:15.016247Z",
     "start_time": "2018-03-08T20:45:15.014275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T20:46:19.149285Z",
     "start_time": "2018-03-08T20:46:19.143003Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 300, 350):\n",
    "    print(u' '.join(trigram_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generating full reviews file\n",
    "\n",
    "Now we will generate the full complete text of reviews which would have normalized text, no stopwords, and second order phrases (trigram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T00:29:46.083706Z",
     "start_time": "2018-03-09T00:29:35.102697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model_filepath)\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Number of reviews in yelp-academic: 991,714  \n",
    "Time taken to write reviews in yelp-academic: 5h 48m 52s  \n",
    "The number of reviews in the original reviews file is 990,627. The trigram transformed reviews have 1,087 reviews more than the original reviews. I am not sure where the increased number of reviews came from.\n",
    "\n",
    "Number of reviews in yelp-full: 3,223,214  \n",
    "Time taken to write reviews in yelp-full: 16h 55m 33s\n",
    "The number of reviews in the original reviews file is 3,221,419. The trigram transformed reviews have 1,795 reviews more than the original reviews. I am not sure where the increased number of reviews came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T06:18:39.829342Z",
     "start_time": "2018-03-09T00:29:47.671374Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_fns import write_trigram_review\n",
    "\n",
    "review_count = write_trigram_review(trigram_reviews_filepath, review_txt_filepath, bigram_model, trigram_model,\n",
    "                                   nlp)\n",
    "print(f'{review_count} reviews written to {trigram_reviews_filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Review File example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:11:13.234149Z",
     "start_time": "2018-03-10T13:11:13.074638Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Visited here on Thursday evening (3/20) around 6:30 pm for dinner.  It was not packed although there were several patrons and more walking in as we were leaving.  No wait time.  I noticed upstairs they had a conference room enclosed with glass walls - looks like it would be a nice place for a birthday party or small private engagement.\n",
      "\n",
      "The food:\n",
      "\n",
      "Bread - The complimentary bread basket had a mixture of parmesan bread sticks and a couple other bread variations.  It came with two bean type dips and olive oil.  The parmesan bread stick is so good!!! I would've hoarded it all if I weren't on a business trip.  Alas, I had to pretend to be civilized and eat only two.  The bread was good, nothing that stood out, but I love bread I'll eat it anyway.\n",
      "\n",
      "Fish of the day - I ordered the fish of the day which was rainbow trout over farro.  It came with some sort of creamy sauce that brought everything together perfectly.  A lot of times, the grains that come with the dish are pretty simple, I felt the farro was seasoned to bring out the flavor and texture of it rather than masking it.  I ate every single bit, it was so good.\n",
      "\n",
      "Dessert - Ordered the biscotti/cookie dessert which came with one scoop of vanilla ice cream with espresso poured over the top in affogato style.  It came with an assortment of cookies, none of which were too sweet.  It came with two of each of Anise and almond biscotti, peanut butter cookie with chopped peanuts, some sort of dark cocoa crispy biscotti type cookie, spice cookie - like ginger molasses, pistachio cookies, and chocolate crinkle cookies. All were pretty small and probably around 1.5\" in diameter.  It would be a good dessert to share, but I don't share and I don't share desserts of all things.  The one cookie I liked the least was the crispy dark cocoa thing, I don't know what it is.  I really liked the spice cookie.  The chocolate crinkle cookie was very fudgy and almost like a brownie in a cookie form. \n",
      "\n",
      "Service was great, although a little pretentious.  Restaurant nice, clean, and upscale, and food came out fast, piping out, and perfect!  Price is fair for the quality.  I would definitely return here!\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "visit thursday_evening 3/20 around_6:30_pm dinner -PRON- pack patron walk -PRON- leave wait time -PRON- notice upstairs -PRON- conference room enclose glass wall look like -PRON- nice place birthday party small private engagement food bread complimentary_bread_basket mixture parmesan bread stick couple bread variation -PRON- come bean type dip olive_oil parmesan bread stick good -PRON- hoard -PRON- -PRON- business trip alas -PRON- pretend civilized eat bread good stand -PRON- love bread -PRON- eat -PRON- fish day -PRON- order fish day rainbow_trout farro -PRON- come sort creamy sauce bring perfectly lot time grain come dish pretty simple -PRON- feel farro season bring flavor texture -PRON- rather_than mask -PRON- -PRON- eat every_single bit -PRON- good dessert order biscotti cookie dessert come scoop vanilla_ice_cream espresso pour affogato style -PRON- come assortment cookie sweet -PRON- come anise almond_biscotti peanut_butter_cookie chop peanut sort dark cocoa crispy biscotti type cookie spice cookie like ginger molass pistachio cookie chocolate crinkle cookie pretty small probably 1.5 diameter -PRON- good dessert share -PRON- share -PRON- share dessert thing cookie -PRON- like crispy dark cocoa thing -PRON- know -PRON- -PRON- like spice cookie chocolate crinkle cookie fudgy like brownie cookie form service great little pretentious restaurant nice clean upscale food come fast pip perfect price fair quality -PRON- definitely return\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helper_fns import line_review\n",
    "\n",
    "print(\"Original:\")\n",
    "print()\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 4352, 4353):\n",
    "    print(review)\n",
    "\n",
    "print(\"----\")\n",
    "print()\n",
    "print(\"Transformed:\")\n",
    "print()\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 4352, 4353):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)\n",
    "\n",
    "We want to put the reviews into different representing different things. The groups are essentially the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary file (once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a full vocabulary of the corpus to be modeled using gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html)  class.\n",
    "\n",
    "Time taken to create yelp-academic dictionary: 1m 12.5s \n",
    "\n",
    "Time taken to create yelp-full dictionary: 3m 5s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T13:39:17.992065Z",
     "start_time": "2018-03-10T13:35:27.833811Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bag-of-words model (once)\n",
    "\n",
    "Using the dictionary created above (which is just a mapping of words to integer ID's we create a bag-of-words model where each review is represented by the coutns of distinct terms in it.\n",
    "\n",
    "Time taken to create and save yelp-academic BOW model:  \n",
    "\n",
    "Time taken to create and save yelp-full BOW model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_fns import trigram_bow_generator\n",
    "\n",
    "MmCorpus.serialize(trigram_bow_filepath,\n",
    "                   trigram_bow_generator(trigram_reviews_filepath, trigram_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
